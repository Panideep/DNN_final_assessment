ğŸ“˜ Cross-Modal Attention Fusion for Sequential Story Generation (TensorFlow Implementation)

This repository contains an end-to-end TensorFlow implementation of a Cross-Modal Attention Fusion architecture designed to improve visualâ€“language alignment in sequential story generation.

The model learns to encode k imageâ€“caption pairs and generate the (k+1)-th caption, using:

CNN-based image encoder

Bi-LSTM text encoder

Cross-modal attention fusion

Temporal LSTM over fused features

Caption decoder

Optional reasoning encoder

This implementation uses HuggingFace datasets, BERT tokenizer, and TensorFlow/Keras for training.

ğŸš€ Features
âœ”ï¸ Cross-Modal Attention Fusion

Fuses spatial CNN features with textual token embeddings using queryâ€“keyâ€“value attention.

âœ”ï¸ Sequential Story Modeling

Temporal LSTM over fused multimodal embeddings for causal story understanding.

âœ”ï¸ (Optional) Reason Encoder

Integrate explanatory or reasoning text into the decoding stage.

âœ”ï¸ TensorFlow Training Loop

Fully custom training with:

GradientTape

Global norm clipping

Step-level logging via tqdm

TF Checkpoints & best-model saving

âœ”ï¸ HuggingFace Dataset Support

Compatible with datasets containing sequences of images + captions (e.g. daniel3303/StoryReasoning).

ğŸ“‚ Project Structure
project/
â”‚
â”œâ”€ src/
â”‚   â”œâ”€ model.py                # TensorFlow cross-modal model
â”‚
â”œâ”€ utils_tf.py                 # TF dataset builder, window generator, misc utils
â”œâ”€ train.py                 # Main training script (can run from terminal)
â”‚
â”œâ”€ config.yaml                 # Dataset & model configuration
â”œâ”€ requirements.txt            # Python dependencies
â”‚
â”œâ”€ README.md                   # (this file)
â””â”€ notebook.ipynb              # Optional Jupyter notebook version

ğŸ“¦ Installation

Create a fresh environment (optional):

python3 -m venv venv
source venv/bin/activate


Install dependencies:

pip install -r requirements.txt


For Apple Silicon:

pip install tensorflow-macos tensorflow-metal

ğŸ§° Configuration (config.yaml)

Your config.yaml controls:

Dataset (paths, sequence length, image size)

Model dimensions (embeddings, attention hidden size)

Training parameters (LR, epochs, batch size)

Dataset keys (e.g. frames, captions, reason)

Example:

dataset:
  hf_name: "daniel3303/StoryReasoning"
  seq_len: 3
  batch_size: 16
  image_size: 128
  max_caption_len: 32
  max_reason_len: 32
  frames_key: "frames"
  captions_key: "captions"
  reason_key: "reason"

model:
  image_feat_dim: 512
  image_spatial_dim: 512
  text_embed_dim: 300
  text_hidden_dim: 512
  multimodal_dim: 512
  temporal_hidden_dim: 512
  vocab_size: 30522
  pad_token_id: 0
  bos_token_id: 101
  eos_token_id: 102
  use_reason_in_fusion: false

training:
  lr: 1e-4
  epochs: 5
  device: "auto"
  log_interval: 50
  save_dir: "results_tf/checkpoints"

ğŸ§ª Training

To train the model:

python train_tf.py --config config.yaml

What training provides:

Step-level loss & perplexity logging

Epoch loss summaries

Automatic checkpoint saving

Automatic best-model weight saving

Optional validation

ğŸ“ˆ Loss Curves

The training script collects:

Train loss per epoch

Validation loss per epoch

Train & validation perplexity

These are plotted automatically:

plt.plot(train_losses)
plt.plot(val_losses)


A loss-vs-epoch chart and optional PPL curve are displayed after training.

ğŸ§  Model Overview
1ï¸âƒ£ Image Encoder

ResNet-like CNN (or TF ConvNet) producing:

Global image feature

Spatial feature map

2ï¸âƒ£ Text Encoder

Bi-LSTM over caption tokens â†’ token-level & sentence-level embeddings.

3ï¸âƒ£ Cross Modal Attention Fusion

Combines spatial visual features with linguistic features:

Q = text tokens
K, V = image patches
Attn = Softmax(QK^T / sqrt(d))


Produces fused multimodal representation per frame.

4ï¸âƒ£ Temporal Encoder

LSTM over fused sequence â†’ contextual embedding.

5ï¸âƒ£ Caption Decoder

Generates the (k+1)-th caption autoregressively using LSTM.

ğŸ“Š Dataset Requirements

Each HuggingFace entry must contain:

"frames" â†’ list of PIL images

"captions" â†’ list of caption strings

"reason" (optional) â†’ explanatory text

Must contain at least k + 1 frames to build windows.

ğŸ§© Customizing

You can easily modify:

Sequence length (seq_len)

Image size (128 â†’ 224)

Add multi-head attention

Replace CNN with ViT

Replace LSTM decoder with Transformer decoder

Add contrastive loss for alignment

If you want help modifying the architecture, just ask.

ğŸ¤ Contributing

Pull requests welcome!
If you'd like additional variants (Transformer-based encoder, CLIP embeddings, ViLT-style fusion), feel free to open an issue.

ğŸ“œ License

MIT License.